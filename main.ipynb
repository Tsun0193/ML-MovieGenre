{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Importing the required libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import warnings\n",
    "\n",
    "from transformers import AutoTokenizer, DistilBertForSequenceClassification\n",
    "from tqdm import tqdm\n",
    "from torchvision import models\n",
    "from torchvision.transforms import v2\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from keras.preprocessing import image\n",
    "from torchmetrics.classification import MultilabelF1Score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setting up the environment\n",
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('ml1m/content/dataset/genres.txt', 'r') as f:\n",
    "    genre_all = f.readlines()\n",
    "genres = [genre.strip() for genre in genre_all]\n",
    "\n",
    "mapping = {}\n",
    "for genre, i in enumerate(genres):\n",
    "    mapping[genre] = i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "users = pd.read_csv('ml1m/content/dataset/users.dat', sep='::',\n",
    "                        engine='python',\n",
    "                        names=['userid', 'gender', 'age', 'occupation', 'zip']).set_index('userid')\n",
    "ratings = pd.read_csv('ml1m/content/dataset/ratings.dat', engine='python',\n",
    "                          sep='::', names=['userid', 'movieid', 'rating', 'timestamp'])\n",
    "movies_train = pd.read_csv('ml1m/content/dataset/movies_train.dat', engine='python',\n",
    "                         sep='::', names=['movieid', 'title', 'genre'], encoding='ISO-8859-1', index_col=False).set_index('movieid')\n",
    "movies_test = pd.read_csv('ml1m/content/dataset/movies_test.dat', engine='python',\n",
    "                         sep='::', names=['movieid', 'title', 'genre'], encoding='ISO-8859-1', index_col=False).set_index('movieid')\n",
    "movies_train['genre'] = movies_train.genre.str.split('|')\n",
    "movies_train.index.name = 'ID'\n",
    "movies_test['genre'] = movies_test.genre.str.split('|')\n",
    "movies_test.index.name = 'ID'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*** \n",
    "Must add overview column to this table via generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(df, path='ml1m/content/dataset/ml1m-images', genres=genres) -> pd.DataFrame:\n",
    "    df['img_path'] = df.apply(lambda x: os.path.join(path, str(x.name) + '.jpg'), axis=1)\n",
    "    df = df[df.img_path.apply(lambda x: os.path.exists(x))]\n",
    "    df['label'] = df.genre.apply(lambda x: [1 if genre in x else 0 for genre in genres])\n",
    "    df.drop(columns=['genre'], inplace=True)\n",
    "    df = df.reset_index(drop=True)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainset = preprocess(movies_train)\n",
    "testset = preprocess(movies_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>img_path</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Washington Square (1997)</td>\n",
       "      <td>ml1m/content/dataset/ml1m-images\\1650.jpg</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Net, The (1995)</td>\n",
       "      <td>ml1m/content/dataset/ml1m-images\\185.jpg</td>\n",
       "      <td>[0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Batman Returns (1992)</td>\n",
       "      <td>ml1m/content/dataset/ml1m-images\\1377.jpg</td>\n",
       "      <td>[1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Boys from Brazil, The (1978)</td>\n",
       "      <td>ml1m/content/dataset/ml1m-images\\3204.jpg</td>\n",
       "      <td>[0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Police Academy 5: Assignment: Miami Beach (1988)</td>\n",
       "      <td>ml1m/content/dataset/ml1m-images\\2382.jpg</td>\n",
       "      <td>[0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              title  \\\n",
       "0                          Washington Square (1997)   \n",
       "1                                   Net, The (1995)   \n",
       "2                             Batman Returns (1992)   \n",
       "3                      Boys from Brazil, The (1978)   \n",
       "4  Police Academy 5: Assignment: Miami Beach (1988)   \n",
       "\n",
       "                                    img_path  \\\n",
       "0  ml1m/content/dataset/ml1m-images\\1650.jpg   \n",
       "1   ml1m/content/dataset/ml1m-images\\185.jpg   \n",
       "2  ml1m/content/dataset/ml1m-images\\1377.jpg   \n",
       "3  ml1m/content/dataset/ml1m-images\\3204.jpg   \n",
       "4  ml1m/content/dataset/ml1m-images\\2382.jpg   \n",
       "\n",
       "                                               label  \n",
       "0  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, ...  \n",
       "1  [0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...  \n",
       "2  [1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, ...  \n",
       "3  [0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...  \n",
       "4  [0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainset.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sub-models\n",
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'pre_classifier.bias', 'classifier.weight', 'pre_classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "You are using a model of type bert to instantiate a model of type distilbert. This is not supported for all configurations of models and can yield errors.\n",
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['transformer.layer.3.attention.q_lin.weight', 'transformer.layer.10.attention.k_lin.bias', 'transformer.layer.9.attention.out_lin.bias', 'transformer.layer.6.attention.k_lin.weight', 'transformer.layer.2.ffn.lin1.weight', 'transformer.layer.10.attention.q_lin.weight', 'transformer.layer.4.output_layer_norm.bias', 'transformer.layer.5.attention.out_lin.bias', 'transformer.layer.7.attention.v_lin.weight', 'transformer.layer.11.sa_layer_norm.weight', 'transformer.layer.2.attention.out_lin.bias', 'transformer.layer.2.output_layer_norm.bias', 'transformer.layer.5.attention.k_lin.bias', 'transformer.layer.11.ffn.lin2.bias', 'transformer.layer.2.output_layer_norm.weight', 'transformer.layer.8.attention.out_lin.bias', 'transformer.layer.4.attention.q_lin.bias', 'transformer.layer.6.attention.q_lin.bias', 'transformer.layer.10.attention.q_lin.bias', 'transformer.layer.10.attention.v_lin.weight', 'transformer.layer.6.ffn.lin1.weight', 'transformer.layer.2.ffn.lin1.bias', 'transformer.layer.0.attention.k_lin.bias', 'transformer.layer.11.output_layer_norm.bias', 'transformer.layer.6.attention.v_lin.weight', 'transformer.layer.8.ffn.lin2.bias', 'transformer.layer.10.sa_layer_norm.weight', 'transformer.layer.6.attention.out_lin.weight', 'pre_classifier.bias', 'transformer.layer.1.output_layer_norm.bias', 'transformer.layer.11.ffn.lin1.weight', 'embeddings.word_embeddings.weight', 'transformer.layer.5.sa_layer_norm.bias', 'transformer.layer.1.attention.k_lin.bias', 'transformer.layer.11.attention.v_lin.bias', 'transformer.layer.0.ffn.lin1.weight', 'transformer.layer.8.attention.v_lin.bias', 'transformer.layer.10.ffn.lin1.bias', 'transformer.layer.3.ffn.lin2.bias', 'transformer.layer.2.sa_layer_norm.weight', 'transformer.layer.2.attention.q_lin.weight', 'embeddings.LayerNorm.bias', 'transformer.layer.10.ffn.lin2.bias', 'transformer.layer.11.output_layer_norm.weight', 'transformer.layer.10.sa_layer_norm.bias', 'transformer.layer.11.attention.q_lin.weight', 'transformer.layer.3.attention.v_lin.bias', 'transformer.layer.11.attention.q_lin.bias', 'transformer.layer.2.attention.v_lin.weight', 'transformer.layer.4.ffn.lin2.bias', 'transformer.layer.1.ffn.lin2.bias', 'transformer.layer.0.sa_layer_norm.bias', 'transformer.layer.6.attention.out_lin.bias', 'transformer.layer.9.attention.q_lin.bias', 'transformer.layer.8.ffn.lin2.weight', 'transformer.layer.0.attention.out_lin.weight', 'transformer.layer.9.sa_layer_norm.bias', 'transformer.layer.7.attention.k_lin.weight', 'transformer.layer.5.ffn.lin1.weight', 'transformer.layer.9.attention.v_lin.bias', 'transformer.layer.2.sa_layer_norm.bias', 'transformer.layer.3.output_layer_norm.bias', 'transformer.layer.4.attention.q_lin.weight', 'transformer.layer.4.ffn.lin2.weight', 'transformer.layer.6.sa_layer_norm.bias', 'transformer.layer.11.sa_layer_norm.bias', 'transformer.layer.6.attention.k_lin.bias', 'transformer.layer.10.attention.out_lin.bias', 'transformer.layer.3.attention.k_lin.weight', 'transformer.layer.10.attention.k_lin.weight', 'transformer.layer.1.output_layer_norm.weight', 'transformer.layer.8.attention.q_lin.weight', 'transformer.layer.3.ffn.lin1.weight', 'transformer.layer.2.ffn.lin2.weight', 'pre_classifier.weight', 'transformer.layer.3.attention.out_lin.bias', 'transformer.layer.4.sa_layer_norm.weight', 'transformer.layer.5.output_layer_norm.weight', 'transformer.layer.0.ffn.lin1.bias', 'transformer.layer.7.ffn.lin2.bias', 'transformer.layer.9.sa_layer_norm.weight', 'transformer.layer.9.ffn.lin1.weight', 'transformer.layer.7.output_layer_norm.bias', 'transformer.layer.6.sa_layer_norm.weight', 'transformer.layer.11.attention.v_lin.weight', 'transformer.layer.7.attention.v_lin.bias', 'transformer.layer.11.attention.out_lin.weight', 'transformer.layer.0.attention.v_lin.weight', 'transformer.layer.1.ffn.lin2.weight', 'transformer.layer.8.output_layer_norm.weight', 'transformer.layer.10.output_layer_norm.weight', 'transformer.layer.8.attention.k_lin.weight', 'transformer.layer.6.output_layer_norm.weight', 'transformer.layer.8.sa_layer_norm.bias', 'transformer.layer.5.ffn.lin1.bias', 'transformer.layer.3.sa_layer_norm.weight', 'transformer.layer.11.ffn.lin2.weight', 'transformer.layer.9.attention.k_lin.bias', 'transformer.layer.1.attention.out_lin.bias', 'transformer.layer.10.attention.out_lin.weight', 'embeddings.position_embeddings.weight', 'transformer.layer.6.output_layer_norm.bias', 'transformer.layer.0.attention.k_lin.weight', 'transformer.layer.11.ffn.lin1.bias', 'classifier.weight', 'transformer.layer.1.attention.v_lin.bias', 'transformer.layer.9.attention.q_lin.weight', 'transformer.layer.5.sa_layer_norm.weight', 'transformer.layer.5.attention.k_lin.weight', 'transformer.layer.1.sa_layer_norm.weight', 'transformer.layer.0.ffn.lin2.weight', 'transformer.layer.7.attention.out_lin.bias', 'transformer.layer.2.attention.out_lin.weight', 'embeddings.LayerNorm.weight', 'transformer.layer.6.ffn.lin2.weight', 'transformer.layer.5.attention.q_lin.weight', 'transformer.layer.1.sa_layer_norm.bias', 'transformer.layer.0.attention.q_lin.bias', 'transformer.layer.3.ffn.lin1.bias', 'transformer.layer.8.attention.q_lin.bias', 'transformer.layer.5.attention.v_lin.weight', 'transformer.layer.0.output_layer_norm.bias', 'transformer.layer.3.attention.q_lin.bias', 'transformer.layer.5.attention.q_lin.bias', 'transformer.layer.7.sa_layer_norm.bias', 'transformer.layer.0.sa_layer_norm.weight', 'transformer.layer.2.attention.k_lin.bias', 'transformer.layer.3.attention.v_lin.weight', 'transformer.layer.3.output_layer_norm.weight', 'transformer.layer.8.attention.v_lin.weight', 'transformer.layer.2.attention.q_lin.bias', 'transformer.layer.2.attention.v_lin.bias', 'transformer.layer.8.sa_layer_norm.weight', 'transformer.layer.6.attention.v_lin.bias', 'transformer.layer.4.ffn.lin1.weight', 'transformer.layer.10.ffn.lin1.weight', 'transformer.layer.4.ffn.lin1.bias', 'classifier.bias', 'transformer.layer.1.attention.v_lin.weight', 'transformer.layer.2.ffn.lin2.bias', 'transformer.layer.2.attention.k_lin.weight', 'transformer.layer.1.attention.k_lin.weight', 'transformer.layer.1.ffn.lin1.bias', 'transformer.layer.1.attention.q_lin.bias', 'transformer.layer.7.attention.q_lin.weight', 'transformer.layer.9.ffn.lin2.bias', 'transformer.layer.6.attention.q_lin.weight', 'transformer.layer.4.attention.v_lin.bias', 'transformer.layer.9.output_layer_norm.bias', 'transformer.layer.4.attention.v_lin.weight', 'transformer.layer.3.attention.k_lin.bias', 'transformer.layer.7.attention.q_lin.bias', 'transformer.layer.4.attention.k_lin.weight', 'transformer.layer.4.attention.out_lin.weight', 'transformer.layer.0.attention.q_lin.weight', 'transformer.layer.10.output_layer_norm.bias', 'transformer.layer.11.attention.k_lin.bias', 'transformer.layer.4.attention.out_lin.bias', 'transformer.layer.3.attention.out_lin.weight', 'transformer.layer.0.attention.out_lin.bias', 'transformer.layer.7.attention.k_lin.bias', 'transformer.layer.10.attention.v_lin.bias', 'transformer.layer.3.sa_layer_norm.bias', 'transformer.layer.5.attention.out_lin.weight', 'transformer.layer.8.ffn.lin1.weight', 'transformer.layer.7.ffn.lin1.weight', 'transformer.layer.0.ffn.lin2.bias', 'transformer.layer.1.attention.q_lin.weight', 'transformer.layer.6.ffn.lin1.bias', 'transformer.layer.7.ffn.lin1.bias', 'transformer.layer.9.attention.v_lin.weight', 'transformer.layer.8.ffn.lin1.bias', 'transformer.layer.11.attention.k_lin.weight', 'transformer.layer.3.ffn.lin2.weight', 'transformer.layer.7.ffn.lin2.weight', 'transformer.layer.11.attention.out_lin.bias', 'transformer.layer.6.ffn.lin2.bias', 'transformer.layer.5.ffn.lin2.bias', 'transformer.layer.9.attention.out_lin.weight', 'transformer.layer.1.ffn.lin1.weight', 'transformer.layer.7.output_layer_norm.weight', 'transformer.layer.9.ffn.lin2.weight', 'transformer.layer.4.sa_layer_norm.bias', 'transformer.layer.10.ffn.lin2.weight', 'transformer.layer.5.attention.v_lin.bias', 'transformer.layer.1.attention.out_lin.weight', 'transformer.layer.5.output_layer_norm.bias', 'transformer.layer.4.attention.k_lin.bias', 'transformer.layer.9.ffn.lin1.bias', 'transformer.layer.7.sa_layer_norm.weight', 'transformer.layer.8.output_layer_norm.bias', 'transformer.layer.7.attention.out_lin.weight', 'transformer.layer.9.output_layer_norm.weight', 'transformer.layer.5.ffn.lin2.weight', 'transformer.layer.8.attention.k_lin.bias', 'transformer.layer.9.attention.k_lin.weight', 'transformer.layer.0.attention.v_lin.bias', 'transformer.layer.0.output_layer_norm.weight', 'transformer.layer.8.attention.out_lin.weight', 'transformer.layer.4.output_layer_norm.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "tokenizer1 = AutoTokenizer.from_pretrained(\"distilbert-base-uncased\")\n",
    "model1 = DistilBertForSequenceClassification.from_pretrained(\"distilbert-base-uncased\", problem_type=\"multi_label_classification\", num_labels=18)\n",
    "model1.config.id2label = mapping\n",
    "\n",
    "tokenizer2 = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "model2 = DistilBertForSequenceClassification.from_pretrained(\"bert-base-uncased\", problem_type=\"multi_label_classification\", num_labels=18)\n",
    "model2.config.id2label = mapping\n",
    "\n",
    "model3 = models.resnet101(pretrained=False)\n",
    "model3.fc = torch.nn.Linear(2048, len(genres))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Deep Fusion Multimodal Model\n",
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Multimodal(torch.nn.Module):\n",
    "    def __init__(self, model1, model2, model3):\n",
    "        super().__init__()\n",
    "        self.model1 = model1\n",
    "        self.model2 = model2\n",
    "        self.model3 = model3\n",
    "        self.fc1 = torch.nn.Linear(18, 18)\n",
    "        self.fc2 = torch.nn.Linear(18, 18)\n",
    "        self.fc3 = torch.nn.Linear(18, 18)\n",
    "\n",
    "    def forward(self, \n",
    "                title_input_ids, title_attention_mask,\n",
    "                plot_input_ids, plot_attention_mask,\n",
    "                image_input):\n",
    "        title_output = self.model1(title_input_ids, title_attention_mask)\n",
    "        plot_output = self.model2(plot_input_ids, plot_attention_mask)\n",
    "        image_output = self.model3(image_input)\n",
    "\n",
    "        title_output = self.fc1(title_output.logits)\n",
    "        plot_output = self.fc2(plot_output.logits)\n",
    "        image_output = self.fc3(image_output)\n",
    "        \n",
    "        output = torch.add(title_output, plot_output, image_output)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Custom Datasets & Data Loaders"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "### Custom Dataset\n",
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Poroset(torch.utils.data.Dataset):\n",
    "    def __init__(self, df, tokenizer1, tokenizer2, max_len=256):\n",
    "        self.df = df\n",
    "        self.tokenizer1 = tokenizer1\n",
    "        self.tokenizer2 = tokenizer2\n",
    "        self.max_len = max_len\n",
    "        self.image = image\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        row = self.df.iloc[idx]\n",
    "        title = row['title']\n",
    "        overview = row['overview']\n",
    "        label = row['label']\n",
    "        title_encoding = self.tokenizer1(title, truncation=True, padding='max_length', max_length=self.max_len, return_tensors='pt')\n",
    "        overview_encoding = self.tokenizer2(overview, truncation=True, padding='max_length', max_length=self.max_len, return_tensors='pt')\n",
    "        image_input = image.open(row['img_path'])\n",
    "        image_input = v2.Compose([v2.Resize((224, 224)), v2.ToTensor()])(image_input)\n",
    "        return {\n",
    "            'title': title,\n",
    "            'overview': overview,\n",
    "            'title_input_ids': title_encoding['input_ids'].squeeze(),\n",
    "            'title_attention_mask': title_encoding['attention_mask'].squeeze(),\n",
    "            'overview_input_ids': overview_encoding['input_ids'].squeeze(),\n",
    "            'overview_attention_mask': overview_encoding['attention_mask'].squeeze(),\n",
    "            'image_input': image_input,\n",
    "            'label': torch.FloatTensor(label)\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainset = Poroset(trainset, tokenizer1, tokenizer2)\n",
    "testset = Poroset(testset, tokenizer1, tokenizer2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "### Custom Data Loader\n",
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainloader = torch.utils.data.DataLoader(trainset, batch_size=32, shuffle=True)\n",
    "testloader = torch.utils.data.DataLoader(testset, batch_size=32, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check if the data loader is working properly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'overview'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "File \u001b[1;32mc:\\Users\\admin\\anaconda3\\Lib\\site-packages\\pandas\\core\\indexes\\base.py:3653\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   3652\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 3653\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_engine\u001b[38;5;241m.\u001b[39mget_loc(casted_key)\n\u001b[0;32m   3654\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n",
      "File \u001b[1;32mc:\\Users\\admin\\anaconda3\\Lib\\site-packages\\pandas\\_libs\\index.pyx:147\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mc:\\Users\\admin\\anaconda3\\Lib\\site-packages\\pandas\\_libs\\index.pyx:176\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mpandas\\_libs\\hashtable_class_helper.pxi:7080\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mpandas\\_libs\\hashtable_class_helper.pxi:7088\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mKeyError\u001b[0m: 'overview'",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[13], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m sample \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mnext\u001b[39m(\u001b[38;5;28miter\u001b[39m(trainloader))\n\u001b[0;32m      2\u001b[0m sample\n",
      "File \u001b[1;32mc:\\Users\\admin\\anaconda3\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:630\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    627\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    628\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[0;32m    629\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[1;32m--> 630\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_data()\n\u001b[0;32m    631\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m    632\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[0;32m    633\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[0;32m    634\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[1;32mc:\\Users\\admin\\anaconda3\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:674\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    672\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    673\u001b[0m     index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m--> 674\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_fetcher\u001b[38;5;241m.\u001b[39mfetch(index)  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m    675\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory:\n\u001b[0;32m    676\u001b[0m         data \u001b[38;5;241m=\u001b[39m _utils\u001b[38;5;241m.\u001b[39mpin_memory\u001b[38;5;241m.\u001b[39mpin_memory(data, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[1;32mc:\\Users\\admin\\anaconda3\\Lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py:51\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[1;34m(self, possibly_batched_index)\u001b[0m\n\u001b[0;32m     49\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[0;32m     50\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m---> 51\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[idx] \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[0;32m     52\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     53\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n",
      "File \u001b[1;32mc:\\Users\\admin\\anaconda3\\Lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py:51\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m     49\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[0;32m     50\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m---> 51\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[idx] \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[0;32m     52\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     53\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n",
      "Cell \u001b[1;32mIn[10], line 15\u001b[0m, in \u001b[0;36mPoroset.__getitem__\u001b[1;34m(self, idx)\u001b[0m\n\u001b[0;32m     13\u001b[0m row \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdf\u001b[38;5;241m.\u001b[39miloc[idx]\n\u001b[0;32m     14\u001b[0m title \u001b[38;5;241m=\u001b[39m row[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtitle\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[1;32m---> 15\u001b[0m overview \u001b[38;5;241m=\u001b[39m row[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124moverview\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[0;32m     16\u001b[0m label \u001b[38;5;241m=\u001b[39m row[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlabel\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[0;32m     17\u001b[0m title_encoding \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtokenizer1(title, truncation\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, padding\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmax_length\u001b[39m\u001b[38;5;124m'\u001b[39m, max_length\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmax_len, return_tensors\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpt\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\admin\\anaconda3\\Lib\\site-packages\\pandas\\core\\series.py:1007\u001b[0m, in \u001b[0;36mSeries.__getitem__\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   1004\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_values[key]\n\u001b[0;32m   1006\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m key_is_scalar:\n\u001b[1;32m-> 1007\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_value(key)\n\u001b[0;32m   1009\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_hashable(key):\n\u001b[0;32m   1010\u001b[0m     \u001b[38;5;66;03m# Otherwise index.get_value will raise InvalidIndexError\u001b[39;00m\n\u001b[0;32m   1011\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1012\u001b[0m         \u001b[38;5;66;03m# For labels that don't resolve as scalars like tuples and frozensets\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\admin\\anaconda3\\Lib\\site-packages\\pandas\\core\\series.py:1116\u001b[0m, in \u001b[0;36mSeries._get_value\u001b[1;34m(self, label, takeable)\u001b[0m\n\u001b[0;32m   1113\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_values[label]\n\u001b[0;32m   1115\u001b[0m \u001b[38;5;66;03m# Similar to Index.get_value, but we do not fall back to positional\u001b[39;00m\n\u001b[1;32m-> 1116\u001b[0m loc \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mindex\u001b[38;5;241m.\u001b[39mget_loc(label)\n\u001b[0;32m   1118\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_integer(loc):\n\u001b[0;32m   1119\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_values[loc]\n",
      "File \u001b[1;32mc:\\Users\\admin\\anaconda3\\Lib\\site-packages\\pandas\\core\\indexes\\base.py:3655\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   3653\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_engine\u001b[38;5;241m.\u001b[39mget_loc(casted_key)\n\u001b[0;32m   3654\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n\u001b[1;32m-> 3655\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(key) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01merr\u001b[39;00m\n\u001b[0;32m   3656\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[0;32m   3657\u001b[0m     \u001b[38;5;66;03m# If we have a listlike key, _check_indexing_error will raise\u001b[39;00m\n\u001b[0;32m   3658\u001b[0m     \u001b[38;5;66;03m#  InvalidIndexError. Otherwise we fall through and re-raise\u001b[39;00m\n\u001b[0;32m   3659\u001b[0m     \u001b[38;5;66;03m#  the TypeError.\u001b[39;00m\n\u001b[0;32m   3660\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_indexing_error(key)\n",
      "\u001b[1;31mKeyError\u001b[0m: 'overview'"
     ]
    }
   ],
   "source": [
    "sample = next(iter(trainloader))\n",
    "sample"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setting up the Trainer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "### GPU & Model Configuration\n",
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model = Multimodal(model1, model2, model3).to(device)\n",
    "\n",
    "# Freeze model2 since it's pretrained\n",
    "for param in model.model2.parameters():\n",
    "    param.requires_grad = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "### Setting up loss function & optimizer\n",
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[17], line 4\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mloss_fn\u001b[39m(outputs, targets):\n\u001b[0;32m      2\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mnn\u001b[38;5;241m.\u001b[39mBCEWithLogitsLoss()(outputs, targets)\n\u001b[1;32m----> 4\u001b[0m optimizer \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39moptim\u001b[38;5;241m.\u001b[39mAdam(params\u001b[38;5;241m=\u001b[39mmodel\u001b[38;5;241m.\u001b[39mparameters(), lr\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1e-5\u001b[39m)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'model' is not defined"
     ]
    }
   ],
   "source": [
    "def loss_fn(outputs, targets):\n",
    "    return torch.nn.BCEWithLogitsLoss()(outputs, targets)\n",
    "\n",
    "optimizer = torch.optim.Adam(params=model.parameters(), lr=1e-4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "### Trainer\n",
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(epoch):\n",
    "    model.train()\n",
    "    for _, data in tqdm(enumerate(trainloader, 0), total=len(trainloader)):\n",
    "        title_input_ids = data['title_input_ids'].to(device)\n",
    "        title_attention_mask = data['title_attention_mask'].to(device)\n",
    "        overview_input_ids = data['overview_input_ids'].to(device)\n",
    "        overview_attention_mask = data['overview_attention_mask'].to(device)\n",
    "        image_input = data['image_input'].to(device)\n",
    "        label = data['label'].to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(title_input_ids, title_attention_mask, overview_input_ids, overview_attention_mask, image_input)\n",
    "        loss = loss_fn(outputs, label)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    print(f'Epoch: {epoch}, Loss:  {loss.item()}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1044/1044 [13:45<00:00,  1.26it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 loss: 0.5877975225448608\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1044/1044 [13:30<00:00,  1.29it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 loss: 0.582400918006897\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1044/1044 [13:30<00:00,  1.29it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 loss: 0.5804471373558044\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1044/1044 [13:30<00:00,  1.29it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 loss: 0.569242000579834\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1044/1044 [13:30<00:00,  1.29it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4 loss: 0.5625030398368835\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1044/1044 [13:31<00:00,  1.29it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5 loss: 0.5552199482917786\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1044/1044 [13:30<00:00,  1.29it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6 loss: 0.5557193756103516\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1044/1044 [13:30<00:00,  1.29it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7 loss: 0.5441743731498718\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1044/1044 [13:30<00:00,  1.29it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8 loss: 0.5739604234695435\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1044/1044 [13:30<00:00,  1.29it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9 loss: 0.549248993396759\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1044/1044 [13:30<00:00,  1.29it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10 loss: 0.5646395683288574\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1044/1044 [13:31<00:00,  1.29it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 11 loss: 0.5462006330490112\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1044/1044 [13:30<00:00,  1.29it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 12 loss: 0.5448351502418518\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1044/1044 [13:30<00:00,  1.29it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 13 loss: 0.5582103133201599\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1044/1044 [13:30<00:00,  1.29it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 14 loss: 0.5416957139968872\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1044/1044 [13:30<00:00,  1.29it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 15 loss: 0.5413329601287842\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1044/1044 [13:30<00:00,  1.29it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 16 loss: 0.5385647416114807\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1044/1044 [13:30<00:00,  1.29it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 17 loss: 0.5503974556922913\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1044/1044 [13:30<00:00,  1.29it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 18 loss: 0.5608019828796387\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1044/1044 [13:30<00:00,  1.29it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 19 loss: 0.553778350353241\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1044/1044 [13:30<00:00,  1.29it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 20 loss: 0.558198869228363\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1044/1044 [13:30<00:00,  1.29it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 21 loss: 0.5483788251876831\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1044/1044 [13:30<00:00,  1.29it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 22 loss: 0.5504338145256042\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1044/1044 [13:30<00:00,  1.29it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 23 loss: 0.5401769280433655\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1044/1044 [13:30<00:00,  1.29it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 24 loss: 0.5429915189743042\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1044/1044 [13:30<00:00,  1.29it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 25 loss: 0.5376887321472168\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1044/1044 [13:29<00:00,  1.29it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 26 loss: 0.5472096800804138\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1044/1044 [13:29<00:00,  1.29it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 27 loss: 0.5329397916793823\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1044/1044 [13:29<00:00,  1.29it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 28 loss: 0.5490307807922363\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1044/1044 [13:29<00:00,  1.29it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 29 loss: 0.5395540595054626\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1044/1044 [13:30<00:00,  1.29it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 30 loss: 0.5566591024398804\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1044/1044 [13:30<00:00,  1.29it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 31 loss: 0.5341386795043945\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(32):\n",
    "    train(epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Validation\n",
    "def validation(testing_loader):\n",
    "    model.eval()\n",
    "    fin_targets = []\n",
    "    fin_outputs = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for _, data in tqdm(enumerate(testing_loader), total=len(testing_loader)):\n",
    "            title_input_ids = data['title_input_ids'].to(device)\n",
    "            title_attention_mask = data['title_attention_mask'].to(device)\n",
    "            overview_input_ids = data['overview_input_ids'].to(device)\n",
    "            overview_attention_mask = data['overview_attention_mask'].to(device)\n",
    "            targets = data['label'].to(device)\n",
    "\n",
    "            outputs = model(title_input_ids, title_attention_mask, overview_input_ids, overview_attention_mask)\n",
    "            fin_targets.extend(targets.cpu().detach().numpy().tolist())\n",
    "            fin_outputs.extend(outputs.cpu().detach().numpy().tolist())\n",
    "\n",
    "    return fin_outputs, fin_targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 185/185 [00:51<00:00,  3.59it/s]\n"
     ]
    }
   ],
   "source": [
    "outputs, targets = validation(testloader)\n",
    "\n",
    "outputs = np.array(outputs) >= 0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.2762)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Multi-label F1 score, macro-averaged\n",
    "from torchmetrics.classification import MultilabelF1Score\n",
    "f1 = MultilabelF1Score(num_labels=18, threshold=0.5, average='macro')\n",
    "f1.update(torch.tensor(outputs), torch.tensor(targets))\n",
    "\n",
    "f1.compute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save model\n",
    "torch.save(model.state_dict(), 'model.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inferencing title and overview\n",
    "def inference(title, overview = None, genres = genres):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        title_encoding = tokenizer(title, truncation=True, padding='max_length', max_length=256, return_tensors='pt')\n",
    "        title_input_ids = title_encoding['input_ids'].to(device)\n",
    "        title_attention_mask = title_encoding['attention_mask'].to(device)\n",
    "        if overview is not None:\n",
    "            overview_encoding = tokenizer(overview, truncation=True, padding='max_length', max_length=256, return_tensors='pt')\n",
    "            overview_input_ids = overview_encoding['input_ids'].to(device)\n",
    "            overview_attention_mask = overview_encoding['attention_mask'].to(device)\n",
    "            outputs = model(title_input_ids, title_attention_mask, overview_input_ids, overview_attention_mask)\n",
    "        else:\n",
    "            outputs = model(title_input_ids, title_attention_mask, None, None)\n",
    "        outputs = outputs.cpu().detach().numpy().tolist()\n",
    "        outputs = np.array(outputs) >= 0.5\n",
    "        print([genres[i] for i in range(len(genres)) if outputs[0][i] == 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a christmas carol\n",
      "Animation\n",
      "Drama\n"
     ]
    }
   ],
   "source": [
    "print(testset[3]['title'])\n",
    "#print genres of the movie in testset in word\n",
    "for i in range(len(genres)):\n",
    "    if testset[3]['label'][i] == 1:\n",
    "        print(genres[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Drama']\n"
     ]
    }
   ],
   "source": [
    "inference(testset[3]['title'],testset[3]['overview'])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.11.5 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "vscode": {
   "interpreter": {
    "hash": "57bc2b6ce032b5f0e93daa91901b7ea38a856826ef43aa9e95b6d3999f5310df"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
