{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "from huggingface_hub import InferenceClient,notebook_login\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: 'Crime',\n",
       " 1: 'Thriller',\n",
       " 2: 'Fantasy',\n",
       " 3: 'Horror',\n",
       " 4: 'Sci-Fi',\n",
       " 5: 'Comedy',\n",
       " 6: 'Documentary',\n",
       " 7: 'Adventure',\n",
       " 8: 'Film-Noir',\n",
       " 9: 'Animation',\n",
       " 10: 'Romance',\n",
       " 11: 'Drama',\n",
       " 12: 'Western',\n",
       " 13: 'Musical',\n",
       " 14: 'Action',\n",
       " 15: 'Mystery',\n",
       " 16: 'War',\n",
       " 17: \"Children's\"}"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open('ml1m/content/dataset/genres.txt', 'r') as f:\n",
    "    genre_all = f.readlines()\n",
    "genres = [genre.strip() for genre in genre_all]\n",
    "\n",
    "mapping = {}\n",
    "for genre, i in enumerate(genres):\n",
    "    mapping[genre] = i\n",
    "\n",
    "mapping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "users = pd.read_csv('ml1m/content/dataset/users.dat', sep='::',\n",
    "                        engine='python',\n",
    "                        names=['userid', 'gender', 'age', 'occupation', 'zip']).set_index('userid')\n",
    "ratings = pd.read_csv('ml1m/content/dataset/ratings.dat', engine='python',\n",
    "                        sep='::', names=['userid', 'movieid', 'rating', 'timestamp'])\n",
    "movies_train = pd.read_csv('ml1m/content/dataset/movies_train.dat', engine='python',\n",
    "                        sep='::', names=['movieid', 'title', 'genre'], encoding='ISO-8859-1', index_col=False).set_index('movieid')\n",
    "movies_test = pd.read_csv('ml1m/content/dataset/movies_test.dat', engine='python',\n",
    "                        sep='::', names=['movieid', 'title', 'genre'], encoding='ISO-8859-1', index_col=False).set_index('movieid')\n",
    "movies_train['genre'] = movies_train.genre.str.split('|')\n",
    "movies_train.index.name = 'ID'\n",
    "movies_test['genre'] = movies_test.genre.str.split('|')\n",
    "movies_test.index.name = 'ID'\n",
    "\n",
    "users.age = users.age.astype('int')\n",
    "users.gender = users.gender.astype('category')\n",
    "users.occupation = users.occupation.astype('category')\n",
    "ratings.movieid = ratings.movieid.astype('int')\n",
    "ratings.userid = ratings.userid.astype('int')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(df, path='ml1m/content/dataset/ml1m-images', genres=genres) -> pd.DataFrame:\n",
    "    df['label'] = df.genre.apply(lambda x: [1 if genre in x else 0 for genre in genres])\n",
    "    df.drop(columns=['genre'], inplace=True)\n",
    "    df.rename(columns={'title': 'text'}, inplace=True)\n",
    "    df = df.reset_index(drop=True)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainset = preprocess(movies_train)\n",
    "trainset['content'] = \"\"\n",
    "testset = preprocess(movies_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "client = InferenceClient(\n",
    "    \"mistralai/Mixtral-8x7B-Instruct-v0.1\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_prompt(message, history):\n",
    "    prompt = \"<s>\"\n",
    "    for user_prompt, bot_response in history:\n",
    "        prompt += f\"[INST] {user_prompt} [/INST]\"\n",
    "        prompt += f\" {bot_response}</s> \"\n",
    "    prompt += f\"[INST] {message} [/INST]\"\n",
    "    return prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate(prompt, history, temperature=0.9, max_new_tokens=256, top_p=0.95, repetition_penalty=1.0):\n",
    "    temperature = float(temperature)\n",
    "    if temperature < 1e-2:\n",
    "        temperature = 1e-2\n",
    "    top_p = float(top_p)\n",
    "\n",
    "    generate_kwargs = dict(\n",
    "        temperature=temperature,\n",
    "        max_new_tokens=max_new_tokens,\n",
    "        top_p=top_p,\n",
    "        repetition_penalty=repetition_penalty,\n",
    "        do_sample=True,\n",
    "        seed=42,\n",
    "    )\n",
    "\n",
    "    formatted_prompt = format_prompt(prompt, history)\n",
    "\n",
    "    stream = client.text_generation(formatted_prompt, **generate_kwargs, stream=True, details=True, return_full_text=False)\n",
    "    output = \"\"\n",
    "\n",
    "    for response in stream:\n",
    "        output += response.token.text\n",
    "        yield output\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "381a0702053544039cfb5232dad047cd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HTML(value='<center> <img\\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.svâ€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "notebook_login()\n",
    "history = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "ename": "HfHubHTTPError",
     "evalue": "429 Client Error: Too Many Requests for url: https://api-inference.huggingface.co/models/mistralai/Mixtral-8x7B-Instruct-v0.1 (Request ID: 7Fu7OOLy3REMHrHlli6d4)\n\nRate limit reached. You reached free usage limit (reset hourly). Please subscribe to a plan at https://huggingface.co/pricing to use the API at this rate",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mHTTPError\u001b[0m                                 Traceback (most recent call last)",
      "File \u001b[1;32mc:\\Users\\Administrator\\anaconda3\\Lib\\site-packages\\huggingface_hub\\utils\\_errors.py:270\u001b[0m, in \u001b[0;36mhf_raise_for_status\u001b[1;34m(response, endpoint_name)\u001b[0m\n\u001b[0;32m    269\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m--> 270\u001b[0m     response\u001b[39m.\u001b[39mraise_for_status()\n\u001b[0;32m    271\u001b[0m \u001b[39mexcept\u001b[39;00m HTTPError \u001b[39mas\u001b[39;00m e:\n",
      "File \u001b[1;32mc:\\Users\\Administrator\\anaconda3\\Lib\\site-packages\\requests\\models.py:1021\u001b[0m, in \u001b[0;36mResponse.raise_for_status\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1020\u001b[0m \u001b[39mif\u001b[39;00m http_error_msg:\n\u001b[1;32m-> 1021\u001b[0m     \u001b[39mraise\u001b[39;00m HTTPError(http_error_msg, response\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m)\n",
      "\u001b[1;31mHTTPError\u001b[0m: 429 Client Error: Too Many Requests for url: https://api-inference.huggingface.co/models/mistralai/Mixtral-8x7B-Instruct-v0.1",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[1;31mHfHubHTTPError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[31], line 11\u001b[0m\n\u001b[0;32m      9\u001b[0m     \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[0;32m     10\u001b[0m _content \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m---> 11\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m token \u001b[38;5;129;01min\u001b[39;00m _out:\n\u001b[0;32m     12\u001b[0m     _content\u001b[38;5;241m.\u001b[39mappend(token)\n\u001b[0;32m     13\u001b[0m trainset\u001b[38;5;241m.\u001b[39mcontent[i] \u001b[38;5;241m=\u001b[39m _content[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m2\u001b[39m]\n",
      "Cell \u001b[1;32mIn[8], line 18\u001b[0m, in \u001b[0;36mgenerate\u001b[1;34m(prompt, history, temperature, max_new_tokens, top_p, repetition_penalty)\u001b[0m\n\u001b[0;32m      7\u001b[0m generate_kwargs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mdict\u001b[39m(\n\u001b[0;32m      8\u001b[0m     temperature\u001b[38;5;241m=\u001b[39mtemperature,\n\u001b[0;32m      9\u001b[0m     max_new_tokens\u001b[38;5;241m=\u001b[39mmax_new_tokens,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     13\u001b[0m     seed\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m42\u001b[39m,\n\u001b[0;32m     14\u001b[0m )\n\u001b[0;32m     16\u001b[0m formatted_prompt \u001b[38;5;241m=\u001b[39m format_prompt(prompt, history)\n\u001b[1;32m---> 18\u001b[0m stream \u001b[38;5;241m=\u001b[39m client\u001b[38;5;241m.\u001b[39mtext_generation(formatted_prompt, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mgenerate_kwargs, stream\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, details\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, return_full_text\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[0;32m     19\u001b[0m output \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     21\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m response \u001b[38;5;129;01min\u001b[39;00m stream:\n",
      "File \u001b[1;32mc:\\Users\\Administrator\\anaconda3\\Lib\\site-packages\\huggingface_hub\\inference\\_client.py:1535\u001b[0m, in \u001b[0;36mInferenceClient.text_generation\u001b[1;34m(self, prompt, details, stream, model, do_sample, max_new_tokens, best_of, repetition_penalty, return_full_text, seed, stop_sequences, temperature, top_k, top_p, truncate, typical_p, watermark, decoder_input_details)\u001b[0m\n\u001b[0;32m   1514\u001b[0m         _set_as_non_tgi(model)\n\u001b[0;32m   1515\u001b[0m         \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtext_generation(  \u001b[39m# type: ignore\u001b[39;00m\n\u001b[0;32m   1516\u001b[0m             prompt\u001b[39m=\u001b[39mprompt,\n\u001b[0;32m   1517\u001b[0m             details\u001b[39m=\u001b[39mdetails,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1533\u001b[0m             decoder_input_details\u001b[39m=\u001b[39mdecoder_input_details,\n\u001b[0;32m   1534\u001b[0m         )\n\u001b[1;32m-> 1535\u001b[0m     raise_text_generation_error(e)\n\u001b[0;32m   1537\u001b[0m \u001b[39m# Parse output\u001b[39;00m\n\u001b[0;32m   1538\u001b[0m \u001b[39mif\u001b[39;00m stream:\n",
      "File \u001b[1;32mc:\\Users\\Administrator\\anaconda3\\Lib\\site-packages\\huggingface_hub\\inference\\_text_generation.py:521\u001b[0m, in \u001b[0;36mraise_text_generation_error\u001b[1;34m(http_error)\u001b[0m\n\u001b[0;32m    518\u001b[0m     \u001b[39mraise\u001b[39;00m exception \u001b[39mfrom\u001b[39;00m \u001b[39mhttp_error\u001b[39;00m\n\u001b[0;32m    520\u001b[0m \u001b[39m# Otherwise, fallback to default error\u001b[39;00m\n\u001b[1;32m--> 521\u001b[0m \u001b[39mraise\u001b[39;00m http_error\n",
      "File \u001b[1;32mc:\\Users\\Administrator\\anaconda3\\Lib\\site-packages\\huggingface_hub\\inference\\_client.py:1511\u001b[0m, in \u001b[0;36mInferenceClient.text_generation\u001b[1;34m(self, prompt, details, stream, model, do_sample, max_new_tokens, best_of, repetition_penalty, return_full_text, seed, stop_sequences, temperature, top_k, top_p, truncate, typical_p, watermark, decoder_input_details)\u001b[0m\n\u001b[0;32m   1509\u001b[0m \u001b[39m# Handle errors separately for more precise error messages\u001b[39;00m\n\u001b[0;32m   1510\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m-> 1511\u001b[0m     bytes_output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpost(json\u001b[39m=\u001b[39mpayload, model\u001b[39m=\u001b[39mmodel, task\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mtext-generation\u001b[39m\u001b[39m\"\u001b[39m, stream\u001b[39m=\u001b[39mstream)  \u001b[39m# type: ignore\u001b[39;00m\n\u001b[0;32m   1512\u001b[0m \u001b[39mexcept\u001b[39;00m HTTPError \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m   1513\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(e, BadRequestError) \u001b[39mand\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39mThe following `model_kwargs` are not used by the model\u001b[39m\u001b[39m\"\u001b[39m \u001b[39min\u001b[39;00m \u001b[39mstr\u001b[39m(e):\n",
      "File \u001b[1;32mc:\\Users\\Administrator\\anaconda3\\Lib\\site-packages\\huggingface_hub\\inference\\_client.py:240\u001b[0m, in \u001b[0;36mInferenceClient.post\u001b[1;34m(self, json, data, model, task, stream)\u001b[0m\n\u001b[0;32m    237\u001b[0m         \u001b[39mraise\u001b[39;00m InferenceTimeoutError(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mInference call timed out: \u001b[39m\u001b[39m{\u001b[39;00murl\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m) \u001b[39mfrom\u001b[39;00m \u001b[39merror\u001b[39;00m  \u001b[39m# type: ignore\u001b[39;00m\n\u001b[0;32m    239\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m--> 240\u001b[0m     hf_raise_for_status(response)\n\u001b[0;32m    241\u001b[0m     \u001b[39mreturn\u001b[39;00m response\u001b[39m.\u001b[39miter_lines() \u001b[39mif\u001b[39;00m stream \u001b[39melse\u001b[39;00m response\u001b[39m.\u001b[39mcontent\n\u001b[0;32m    242\u001b[0m \u001b[39mexcept\u001b[39;00m HTTPError \u001b[39mas\u001b[39;00m error:\n",
      "File \u001b[1;32mc:\\Users\\Administrator\\anaconda3\\Lib\\site-packages\\huggingface_hub\\utils\\_errors.py:330\u001b[0m, in \u001b[0;36mhf_raise_for_status\u001b[1;34m(response, endpoint_name)\u001b[0m\n\u001b[0;32m    326\u001b[0m     \u001b[39mraise\u001b[39;00m BadRequestError(message, response\u001b[39m=\u001b[39mresponse) \u001b[39mfrom\u001b[39;00m \u001b[39me\u001b[39;00m\n\u001b[0;32m    328\u001b[0m \u001b[39m# Convert `HTTPError` into a `HfHubHTTPError` to display request information\u001b[39;00m\n\u001b[0;32m    329\u001b[0m \u001b[39m# as well (request id and/or server error message)\u001b[39;00m\n\u001b[1;32m--> 330\u001b[0m \u001b[39mraise\u001b[39;00m HfHubHTTPError(\u001b[39mstr\u001b[39m(e), response\u001b[39m=\u001b[39mresponse) \u001b[39mfrom\u001b[39;00m \u001b[39me\u001b[39;00m\n",
      "\u001b[1;31mHfHubHTTPError\u001b[0m: 429 Client Error: Too Many Requests for url: https://api-inference.huggingface.co/models/mistralai/Mixtral-8x7B-Instruct-v0.1 (Request ID: 7Fu7OOLy3REMHrHlli6d4)\n\nRate limit reached. You reached free usage limit (reset hourly). Please subscribe to a plan at https://huggingface.co/pricing to use the API at this rate"
     ]
    }
   ],
   "source": [
    "_template = \"summarize the movie: \"\n",
    "for i in range(299,len(trainset)):\n",
    "    _in = trainset.text[i]\n",
    "    _out = _template + _in\n",
    "    try:\n",
    "        _out = generate(_out, history)\n",
    "    except Exception as e:\n",
    "        print(\"Paused at movie \" + str(i))\n",
    "        break\n",
    "    _content = []\n",
    "    for token in _out:\n",
    "        _content.append(token)\n",
    "    trainset.content[i] = _content[-2]\n",
    "    if i%60==0:\n",
    "        print(\"Generated \" + str(i) + \"movies\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " \"Turbulence\" is a 1997 action thriller film directed by Robert Butler. The movie follows the story of a hijacked plane that gets diverted due to bad weather, leading to a tense and thrilling situation on board.\n",
      "\n",
      "The film starts with Ryan Weaver, a serial killer, being transported on a commercial plane. However, things go awry when the plane hits turbulence, and the prisoners manage to break free. In the ensuing chaos, the pilots are killed, and the plane is left without anyone to fly it.\n",
      "\n",
      "Meanwhile, on the ground, Stacy, a flight attendant who managed to escape the plane during the hijacking, tries to alert the authorities about the situation on board. She teams up with a disgraced air traffic controller, Terry, to help her take control of the plane.\n",
      "\n",
      "Back on the plane, Ryan takes charge, manipulating and killing some of the passengers in the process. However, he soon realizes that Stacy and Terry are working together to retake control of the aircraft.\n",
      "\n",
      "In a tense and thrilling finale, Stacy and Terry manage to regain control of the plane, with Stacy sacrificing herself\n"
     ]
    }
   ],
   "source": [
    "print(trainset.content[299])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.11.5 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "57bc2b6ce032b5f0e93daa91901b7ea38a856826ef43aa9e95b6d3999f5310df"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
